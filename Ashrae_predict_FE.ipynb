{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ashrae_predict_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurence-lin/Kaggle_competition/blob/master/Ashrae_predict_FE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Vn9mfylThK",
        "colab_type": "code",
        "outputId": "db27cb2f-2037-4afd-8b1b-cb584ee98b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import sklearn\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import gc\n",
        "from google.colab import files\n",
        "# load data from Cloud Storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Configure GCP project and use gsutil to copy the file from storage\n",
        "\n",
        "!gcloud config set project 'blind-detection'\n",
        "!gsutil -m cp -r gs://ashare_dataset/*.csv  sample_data/\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://ashare_dataset/building_metadata.csv...\n",
            "Copying gs://ashare_dataset/sample_submission.csv...\n",
            "Copying gs://ashare_dataset/test.csv...\n",
            "Copying gs://ashare_dataset/weather_test.csv...\n",
            "Copying gs://ashare_dataset/train.csv...\n",
            "Copying gs://ashare_dataset/weather_train.csv...\n",
            "Resuming download for sample_data/test.csv component 0\n",
            "Resuming download for sample_data/sample_submission.csv component 2\n",
            "Resuming download for sample_data/train.csv component 1\n",
            "Resuming download for sample_data/test.csv component 1\n",
            "Resuming download for sample_data/sample_submission.csv component 0\n",
            "Resuming download for sample_data/train.csv component 0\n",
            "Resuming download for sample_data/test.csv component 2\n",
            "Resuming download for sample_data/sample_submission.csv component 1\n",
            "Resuming download for sample_data/train.csv component 2\n",
            "Resuming download for sample_data/test.csv component 3\n",
            "Resuming download for sample_data/train.csv component 3\n",
            "\\ [6/6 files][  2.4 GiB/  2.4 GiB] 100% Done  34.9 MiB/s ETA 00:00:00           \n",
            "Operation completed over 6 objects/2.4 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKLd8hpOV4m",
        "colab_type": "code",
        "outputId": "58c566db-4f71-48fa-d483-03163e39d810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "# Reduce memory function\n",
        "\n",
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16\n",
        "\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Load all datasets and reduce memory\n",
        "print(os.listdir('sample_data/'))\n",
        "data_path = 'sample_data/'\n",
        "train = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates = ['timestamp'])\n",
        "test = pd.read_csv(os.path.join(data_path, 'test.csv'), parse_dates = ['timestamp'])\n",
        "building = pd.read_csv(os.path.join(data_path, 'building_metadata.csv'))\n",
        "weather_test = pd.read_csv(os.path.join(data_path, 'weather_test.csv'), parse_dates = ['timestamp'])\n",
        "#submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\n",
        "weather_train = pd.read_csv(os.path.join(data_path, 'weather_train.csv'), parse_dates = ['timestamp'])\n",
        "\n",
        "train = reduce_mem_usage(train, use_float16 = True)\n",
        "building = reduce_mem_usage(building, use_float16 = True)\n",
        "weather_train = reduce_mem_usage(weather_train, use_float16 = True)\n",
        "test = reduce_mem_usage(test)\n",
        "weather_test = reduce_mem_usage(weather_test)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anscombe.json', 'README.md', 'weather_train.csv', 'train.csv', 'sample_submission.csv', 'building_metadata.csv', 'test.csv', 'weather_test.csv', 'california_housing_test.csv', 'mnist_train_small.csv', 'california_housing_train.csv', 'mnist_test.csv']\n",
            "Memory usage of dataframe is 616.95 MB\n",
            "Memory usage after optimization is: 289.19 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 0.07 MB\n",
            "Memory usage after optimization is: 0.02 MB\n",
            "Decreased by 73.8%\n",
            "Memory usage of dataframe is 9.60 MB\n",
            "Memory usage after optimization is: 3.07 MB\n",
            "Decreased by 68.1%\n",
            "Memory usage of dataframe is 1272.51 MB\n",
            "Memory usage after optimization is: 596.49 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 19.04 MB\n",
            "Memory usage after optimization is: 9.78 MB\n",
            "Decreased by 48.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vByNpOtOHUL0",
        "colab_type": "text"
      },
      "source": [
        "1. Load dataset \n",
        "2. Do EDA to analyze data structure \n",
        "3. Do feature engineering \n",
        "4. Apply model training \n",
        "5. Make test data prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXBxv4GcLzxQ",
        "colab_type": "code",
        "outputId": "cc5cf13f-066b-4fb6-b1af-aeda3d376c6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Do interpolation for weather data first, for too much missing values. There may still be some missing values after this.\n",
        "# Interpolate by each site across the timestamp\n",
        "weather_train = weather_train.groupby('site_id').apply(lambda x_site: x_site.interpolate(limit_direction = 'both'))\n",
        "weather_test = weather_test.groupby('site_id').apply(lambda x_site: x_site.interpolate(limit_direction = 'both'))\n",
        "\n",
        "print('Missing values in weather train data after interpolation: \\n')\n",
        "print(weather_train.isnull().sum().sort_values(ascending = False))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing values in weather train data after interpolation: \n",
            "\n",
            "precip_depth_1_hr     26273\n",
            "cloud_coverage        17228\n",
            "sea_level_pressure     8755\n",
            "wind_speed                0\n",
            "wind_direction            0\n",
            "dew_temperature           0\n",
            "air_temperature           0\n",
            "timestamp                 0\n",
            "site_id                   0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51enM79EN3Wv",
        "colab_type": "text"
      },
      "source": [
        "I can see that although some NaN values is filled in weather_data, interpolation couldn't fill all of the missing values.  \n",
        "We will continue this in FE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24BJExaFK7Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine data and meta datas for advanced analysis\n",
        "train = train.merge(building, on = 'building_id', how = 'left')\n",
        "train = train.merge(weather_train, on = ['site_id', 'timestamp'], how = 'left')\n",
        "test = test.merge(building, on = 'building_id', how = 'left')\n",
        "test = test.merge(weather_test, on = ['site_id', 'timestamp'], how = 'left')\n",
        "\n",
        "del building, weather_train, weather_test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCgTu8IyPwON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After the timestamp alignment, new columns 'offset' may generate whole row that is NaN. Drop these rows\n",
        "# Which means whole row with all NaN values\n",
        "weather_col = ['air_temperature', 'dew_temperature', 'wind_speed', \\\n",
        "               'wind_direction', 'offset', 'sea_level_pressure',\n",
        "               'precip_depth_1_hr', 'cloud_coverage']\n",
        "train = train.dropna(subset = weather_col, how = 'all')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smTxW2hYVxM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "e40ad1ee-99e1-4fbc-8a7d-7136d317314e"
      },
      "source": [
        "# Fill NaNs\n",
        "null_col = 100 - train.count()/len(train)*100\n",
        "null_col = train.loc[:, null_col > 0] # dataframe from train that contain null columns\n",
        "\n",
        "def mean_without_overflow_fast(col):\n",
        "    # Compute mean value of each column which contains missing value\n",
        "    col /= len(col)\n",
        "    return col.mean() * len(col)\n",
        "\n",
        "null_col_mean = null_col.apply(mean_without_overflow_fast)\n",
        "\n",
        "for col in null_col.keys():\n",
        "  # Fill mean with integer for year_built and floor_count\n",
        "   if col == 'year_built' or col =='floor_count':\n",
        "     train[col].fillna(math.floor(null_col_mean[col]), inplace = True)\n",
        "     test[col].fillna(math.floor(null_col_mean[col]), inplace = True)\n",
        "   else:\n",
        "     train[col].fillna(null_col_mean[col], inplace = True)\n",
        "     test[col].fillna(null_col_mean[col], inplace = True)\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:6287: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._update_inplace(new_data)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6RxctSDrOO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "bb5d011a-767a-4a82-f8a9-7d99a9ec0c91"
      },
      "source": [
        "# Create new feature: feature generation\n",
        "\n",
        "def time_transform(df):\n",
        "  df['hour'] = df['timestamp'].dt.hour\n",
        "  df['year'] = df['timestamp'].dt.year\n",
        "  df['month'] = df.timestamp.dt.month\n",
        "  df['day'] = df.timestamp.dt.day\n",
        "  df['dayofweek'] = df.timestamp.dt.dayofweek\n",
        "  \n",
        "  return df\n",
        "\n",
        "train = time_transform(train)\n",
        "test = time_transform(test)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNnWCE5nv2RC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create is_holiday feature by US_Holiday calendar\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
        "\n",
        "# Add is_holiday = 1: The date that within USA_holiday, and the dates that is weekend\n",
        "\n",
        "date_range = pd.date_range(start = train['timestamp'].min(), end = test['timestamp'].max())\n",
        "us_holidays = calendar().holidays(start = date_range.min(), end = date_range.max()) # USA holidays within data date_range\n",
        "# Only datetime64 could apply isin() function, which is convenient\n",
        "train['is_holiday'] = (train['timestamp'].dt.date).astype('datetime64').isin(us_holidays).astype(np.int8)\n",
        "test['is_holiday'] = (test['timestamp'].dt.date).astype('datetime64').isin(us_holidays).astype(np.int8)\n",
        "\n",
        "train.loc[(train['timestamp'].dt.dayofweek == 5) | (train['timestamp'].dt.dayofweek == 6), 'is_holiday'] = 1\n",
        "test.loc[(test['timestamp'].dt.dayofweek == 5) | (test['timestamp'].dt.dayofweek == 6), 'is_holiday'] = 1\n",
        "\n",
        "\n",
        "gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxRKgp-QaW1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode cyclic features\n",
        "def encode_cyclic_feature(df, col, max_val):\n",
        "  '''\n",
        "  Encode cyclic feature with sin cosine transform\n",
        "  df: dataframe contains cyclic feature\n",
        "  col: cylcic features to transform\n",
        "  max_val: max value for that cyclic column\n",
        "  '''\n",
        "  df[col + '_sin'] = np.sin(2*np.pi*(df[col]/max_val))\n",
        "  del df[col]\n",
        "  return df\n",
        "\n",
        "train = encode_cyclic_feature(train, 'dayofweek', 7)\n",
        "train = encode_cyclic_feature(train, 'hour', 24)\n",
        "train = encode_cyclic_feature(train, 'day', 31)\n",
        "train = encode_cyclic_feature(train, 'month', 31)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Lk0Xl6dfJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data cleaning: drop useless features and samples\n",
        "train.corr()['meter_reading'].plot()\n",
        "\n",
        "drop_features = ['wind_speed', 'sea_level_pressure']\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMs63obQid7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature engineering\n",
        "\n",
        "def preprocessing(df, building, weather, is_train = False):\n",
        "  '''\n",
        "  df: train data or test dataframe\n",
        "  '''\n",
        "\n",
        "  # 1. Log transformation for training target\n",
        "  if is_train:\n",
        "    df['meter_reading'] = np.log1p(df['meter_reading'])\n",
        "  \n",
        "  df['square_feet'] = np.log1p(df['square_feet'])\n",
        "\n",
        "  # 2. Fill NaNs\n",
        "  \n",
        "\n",
        "  # 3. Categorical encoding\n",
        "\n",
        "  # 4. Data cleaning: NaN rows, Outliers, ...etc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}