{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ashrae_predict_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurence-lin/Kaggle_competition/blob/master/Ashrae_predict_FE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Vn9mfylThK",
        "colab_type": "code",
        "outputId": "df3d27ac-4c16-433a-97aa-31c7ecab3946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import sklearn\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import gc\n",
        "from google.colab import files\n",
        "# load data from Cloud Storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Configure GCP project and use gsutil to copy the file from storage\n",
        "\n",
        "!gcloud config set project 'blind-detection'\n",
        "!gsutil -m cp -r gs://ashare_dataset/*.csv  sample_data/\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://ashare_dataset/building_metadata.csv...\n",
            "Copying gs://ashare_dataset/sample_submission.csv...\n",
            "Copying gs://ashare_dataset/test.csv...\n",
            "Copying gs://ashare_dataset/train.csv...\n",
            "Copying gs://ashare_dataset/weather_test.csv...\n",
            "Copying gs://ashare_dataset/weather_train.csv...\n",
            "- [6/6 files][  2.4 GiB/  2.4 GiB] 100% Done  58.6 MiB/s ETA 00:00:00           \n",
            "Operation completed over 6 objects/2.4 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKLd8hpOV4m",
        "colab_type": "code",
        "outputId": "0c93052f-081f-445b-8e11-ab89e96e70bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "# Reduce memory function\n",
        "\n",
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16\n",
        "\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Load all datasets and reduce memory\n",
        "print(os.listdir('sample_data/'))\n",
        "data_path = 'sample_data/'\n",
        "train = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates = ['timestamp'])\n",
        "test = pd.read_csv(os.path.join(data_path, 'test.csv'), parse_dates = ['timestamp'])\n",
        "building = pd.read_csv(os.path.join(data_path, 'building_metadata.csv'))\n",
        "weather_test = pd.read_csv(os.path.join(data_path, 'weather_test.csv'), parse_dates = ['timestamp'])\n",
        "#submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\n",
        "weather_train = pd.read_csv(os.path.join(data_path, 'weather_train.csv'), parse_dates = ['timestamp'])\n",
        "\n",
        "train = reduce_mem_usage(train, use_float16 = True)\n",
        "building = reduce_mem_usage(building, use_float16 = True)\n",
        "weather_train = reduce_mem_usage(weather_train, use_float16 = True)\n",
        "test = reduce_mem_usage(test)\n",
        "weather_test = reduce_mem_usage(weather_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anscombe.json', 'README.md', 'weather_train.csv', 'train.csv', 'sample_submission.csv', 'building_metadata.csv', 'test.csv', 'weather_test.csv', 'california_housing_test.csv', 'mnist_train_small.csv', 'california_housing_train.csv', 'mnist_test.csv']\n",
            "Memory usage of dataframe is 616.95 MB\n",
            "Memory usage after optimization is: 289.19 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 0.07 MB\n",
            "Memory usage after optimization is: 0.02 MB\n",
            "Decreased by 73.8%\n",
            "Memory usage of dataframe is 9.60 MB\n",
            "Memory usage after optimization is: 3.07 MB\n",
            "Decreased by 68.1%\n",
            "Memory usage of dataframe is 1272.51 MB\n",
            "Memory usage after optimization is: 596.49 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 19.04 MB\n",
            "Memory usage after optimization is: 9.78 MB\n",
            "Decreased by 48.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vByNpOtOHUL0",
        "colab_type": "text"
      },
      "source": [
        "1. Load dataset \n",
        "2. Do EDA to analyze data structure \n",
        "3. Do feature engineering \n",
        "4. Apply model training \n",
        "5. Make test data prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXBxv4GcLzxQ",
        "colab_type": "code",
        "outputId": "3da68802-c19b-44c3-f635-dbbc8d511ec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "source": [
        "# Time alignment\n",
        "\n",
        "temp_skeleton = pd.concat([weather_train, weather_test], ignore_index = True)\n",
        "weather_key = ['site_id', 'timestamp']\n",
        "# Drop samples with same site and same timestamp\n",
        "temp_skeleton = temp_skeleton[weather_key + ['air_temperature']].drop_duplicates( \\\n",
        "                              subset = weather_key).sort_values(by = weather_key)\n",
        "\n",
        "# Ranking of temperature in each date, at each site\n",
        "temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton['timestamp'].dt.date]) \\\n",
        "                        ['air_temperature'].rank(method = 'average')\n",
        "\n",
        "# Create a dataframe that consisted of: site_id * hourly mean rank of temperature for 24 hours\n",
        "df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level = 1)\n",
        "\n",
        "# Subtract max temperature hourly rank by 14, get time alignment gap: time gap btw 14:00 and peak temp. timing\n",
        "site_offset = pd.Series(df_2d.values.argmax(axis = 1) - 14)\n",
        "site_offset.index.name = 'site_id'\n",
        "\n",
        "def time_align(df):\n",
        "  # create time offset column\n",
        "  df['offset'] = df.site_id.map(site_offset)\n",
        "  df['timestamp_aligned'] = df.timestamp - pd.to_timedelta(df.offset, unit = 'hour')\n",
        "  df['timestamp'] = df['timestamp_aligned']\n",
        "  del df['timestamp_aligned']\n",
        "  return df\n",
        "\n",
        "# Now, we can align weather_train, weather_test data\n",
        "weather_train = time_align(weather_train)\n",
        "weather_test = time_align(weather_test)\n",
        "\n",
        "del df_2d, temp_skeleton, site_offset\n",
        "\n",
        "# Do interpolation for weather data first, for too much missing values. There may still be some missing values after this.\n",
        "# Interpolate by each site across the timestamp\n",
        "weather_train = weather_train.groupby('site_id').apply(lambda x_site: x_site.interpolate(limit_direction = 'both'))\n",
        "weather_test = weather_test.groupby('site_id').apply(lambda x_site: x_site.interpolate(limit_direction = 'both'))\n",
        "\n",
        "print('Missing values in weather train data after interpolation: \\n')\n",
        "print(weather_train.isnull().sum().sort_values(ascending = False))\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing values in weather train data after interpolation: \n",
            "\n",
            "precip_depth_1_hr     26273\n",
            "cloud_coverage        17228\n",
            "sea_level_pressure     8755\n",
            "offset                    0\n",
            "wind_speed                0\n",
            "wind_direction            0\n",
            "dew_temperature           0\n",
            "air_temperature           0\n",
            "timestamp                 0\n",
            "site_id                   0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51enM79EN3Wv",
        "colab_type": "text"
      },
      "source": [
        "I can see that although some NaN values is filled in weather_data, interpolation couldn't fill all of the missing values.  \n",
        "We will continue this in FE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxRKgp-QaW1a",
        "colab_type": "code",
        "outputId": "64ec311d-dc55-46f5-e135-2a1910373658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Functions for several preprocessing and feature engineering\n",
        "\n",
        "# Encode cyclic features\n",
        "def encode_cyclic_feature(df, col, max_val):\n",
        "  '''\n",
        "  Encode cyclic feature with sin cosine transform\n",
        "  df: dataframe contains cyclic feature\n",
        "  col: cylcic features to transform\n",
        "  max_val: max value for that cyclic column\n",
        "  '''\n",
        "  df[col + '_sin'] = np.sin(2*np.pi*(df[col]/max_val))\n",
        "  del df[col]\n",
        "  return df\n",
        "\n",
        "# Fill NaNs\n",
        "def mean_without_overflow_fast(col):\n",
        "    # Compute mean value of each column which contains missing value\n",
        "    col /= len(col)\n",
        "    return col.mean() * len(col)\n",
        "\n",
        "def fillna(df):\n",
        "  '''\n",
        "  Fill NaN for dataframe \n",
        "  output: dataframe without missing values\n",
        "  '''\n",
        "  # pick up the columns contains null value\n",
        "  null_col = 100 - df.count()/len(df)*100\n",
        "  null_col = df.loc[:, null_col > 0] # dataframe from train that contain null columns\n",
        "  null_col_mean = null_col.apply(mean_without_overflow_fast) # mean value to fill in\n",
        "\n",
        "  for col in null_col.keys():\n",
        "    if col == 'year_built' or col == 'floor_count':\n",
        "      df[col].fillna(math.floor(null_col_mean[col]), inplace = True)\n",
        "    else:\n",
        "      df[col].fillna(null_col_mean[col], inplace = True)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create new feature\n",
        "# Time stamp feature\n",
        "def time_transform(df):\n",
        "  df['hour'] = df['timestamp'].dt.hour\n",
        "  df['year'] = df['timestamp'].dt.year\n",
        "  df['month'] = df.timestamp.dt.month\n",
        "  df['day'] = df.timestamp.dt.day\n",
        "  df['dayofweek'] = df.timestamp.dt.dayofweek\n",
        "  \n",
        "  return df\n",
        "\n",
        "# Create is_holiday feature by US_Holiday calendar\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
        "\n",
        "# Add is_holiday = 1: The date that within USA_holiday, and the dates that is weekend\n",
        "\n",
        "date_range = pd.date_range(start = train['timestamp'].min(), end = test['timestamp'].max())\n",
        "us_holidays = calendar().holidays(start = date_range.min(), end = date_range.max()) # USA holidays within data date_range\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LDtW7tsUq0K",
        "colab_type": "code",
        "outputId": "84b37be5-b8c7-42e1-ac78-9792ac7ca943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(train.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['building_id', 'meter', 'timestamp', 'meter_reading'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMs63obQid7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature engineering\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "def preprocessing(df, building, weather, is_train = False):\n",
        "  '''\n",
        "  df: train data or test dataframe\n",
        "  '''\n",
        "\n",
        "  # 0. Combine the dataset into one\n",
        "  df = df.merge(building, on = 'building_id', how = 'left')\n",
        "  df = df.merge(weather, on = ['site_id', 'timestamp'], how = 'left')\n",
        "\n",
        "  weather_col = ['air_temperature', 'dew_temperature', 'wind_speed', \\\n",
        "               'wind_direction', 'sea_level_pressure',\n",
        "               'precip_depth_1_hr', 'cloud_coverage']\n",
        "  df = df.dropna(subset = weather_col, how = 'all')\n",
        "\n",
        "  # 1. Create new features\n",
        "  df = time_transform(df)\n",
        "\n",
        "  # Only datetime64 could apply isin() function, which is convenient\n",
        "  df['is_holiday'] = (df['timestamp'].dt.date).astype('datetime64').isin(us_holidays).astype(np.int8)\n",
        "  df.loc[(df['timestamp'].dt.dayofweek == 5) | (df['timestamp'].dt.dayofweek == 6), 'is_holiday'] = 1\n",
        "\n",
        "  # 2. Data transformation to make data better for prediction\n",
        "  # Log transformation for numerical data\n",
        "  if is_train:\n",
        "    df['meter_reading'] = np.log1p(df['meter_reading'])\n",
        "  \n",
        "  df['square_feet'] = np.log1p(df['square_feet'])\n",
        "\n",
        "  # Encode cyclic features\n",
        "  df = encode_cyclic_feature(df, 'dayofweek', 7)\n",
        "  df = encode_cyclic_feature(df, 'hour', 24)\n",
        "  df = encode_cyclic_feature(df, 'day', 31)\n",
        "  df = encode_cyclic_feature(df, 'month', 31)\n",
        "  \n",
        "  # 3. Fill NaNs\n",
        "  df = fillna(df)\n",
        "\n",
        "  # 4. Categorical encoding\n",
        "  df['primary_use'] = le.fit_transform(df['primary_use'])\n",
        "\n",
        "  # 5. Data cleaning: NaN rows, Outliers, ...etc\n",
        "  drop_features = ['wind_speed', 'sea_level_pressure', 'wind_direction', 'timestamp']\n",
        "  df.drop(drop_features, axis = 1, inplace = True)\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnMVmJvdFQ56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = preprocessing(train, building, weather_train, is_train = True)\n",
        "test = preprocessing(test, building, weather_test, is_train = False)\n",
        "\n",
        "del building, weather_train, weather_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmHIUlfTTB72",
        "colab_type": "code",
        "outputId": "19ebcd4b-885e-4918-a553-ca81c80c2098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        }
      },
      "source": [
        "print('Train data shape: ', train.shape)\n",
        "print(train.info())\n",
        "print('Test data shape: ', test.shape)\n",
        "print(test.info())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (20112649, 19)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 20112649 entries, 0 to 20215255\n",
            "Data columns (total 19 columns):\n",
            "building_id          int16\n",
            "meter                int8\n",
            "meter_reading        float32\n",
            "site_id              int8\n",
            "primary_use          int64\n",
            "square_feet          float64\n",
            "year_built           float16\n",
            "floor_count          float16\n",
            "air_temperature      float16\n",
            "cloud_coverage       float16\n",
            "dew_temperature      float16\n",
            "precip_depth_1_hr    float16\n",
            "offset               float64\n",
            "year                 int64\n",
            "is_holiday           int8\n",
            "dayofweek_sin        float64\n",
            "hour_sin             float64\n",
            "day_sin              float64\n",
            "month_sin            float64\n",
            "dtypes: float16(6), float32(1), float64(6), int16(1), int64(2), int8(3)\n",
            "memory usage: 1.7 GB\n",
            "None\n",
            "Test data shape:  (41484919, 19)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 41484919 entries, 0 to 41697599\n",
            "Data columns (total 19 columns):\n",
            "row_id               int32\n",
            "building_id          int16\n",
            "meter                int8\n",
            "site_id              int8\n",
            "primary_use          int64\n",
            "square_feet          float64\n",
            "year_built           float16\n",
            "floor_count          float16\n",
            "air_temperature      float32\n",
            "cloud_coverage       float32\n",
            "dew_temperature      float32\n",
            "precip_depth_1_hr    float32\n",
            "offset               float64\n",
            "year                 int64\n",
            "is_holiday           int8\n",
            "dayofweek_sin        float64\n",
            "hour_sin             float64\n",
            "day_sin              float64\n",
            "month_sin            float64\n",
            "dtypes: float16(2), float32(4), float64(6), int16(1), int32(1), int64(2), int8(3)\n",
            "memory usage: 3.9 GB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccRe5dT_Pl-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv('train_processed.csv', index = False)\n",
        "test.to_csv('test_processed.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}