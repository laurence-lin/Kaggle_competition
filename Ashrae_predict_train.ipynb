{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ashrae_predict_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurence-lin/Kaggle_competition/blob/master/Ashrae_predict_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Vn9mfylThK",
        "colab_type": "code",
        "outputId": "be1764fa-66a7-4f58-9370-ba8e7ce334a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import sklearn\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import gc\n",
        "from google.colab import files\n",
        "# load data from Cloud Storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!pip install mlxtend\n",
        "from mlxtend.regressor import StackingRegressor\n",
        "\n",
        "# Configure GCP project and use gsutil to copy the file from storage\n",
        "\n",
        "!gcloud config set project 'blind-detection'\n",
        "!gsutil -m cp -r gs://ashare_dataset/*.csv  sample_data/\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.17.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.22.1)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (42.0.2)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->mlxtend) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.17.1->mlxtend) (1.12.0)\n",
            "Updated property [core/project].\n",
            "Copying gs://ashare_dataset/sample_submission.csv...\n",
            "Copying gs://ashare_dataset/building_metadata.csv...\n",
            "Copying gs://ashare_dataset/train.csv...\n",
            "Copying gs://ashare_dataset/weather_train.csv...\n",
            "Copying gs://ashare_dataset/weather_test.csv...\n",
            "Copying gs://ashare_dataset/test.csv...\n",
            "/ [6/6 files][  2.4 GiB/  2.4 GiB] 100% Done  48.9 MiB/s ETA 00:00:00           \n",
            "Operation completed over 6 objects/2.4 GiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKLd8hpOV4m",
        "colab_type": "code",
        "outputId": "211969d8-6068-41ec-f3f1-70f729a77eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# Reduce memory function\n",
        "\n",
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16\n",
        "\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Load all datasets and reduce memory\n",
        "print(os.listdir('sample_data/'))\n",
        "data_path = 'sample_data/'\n",
        "train = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates = ['timestamp'])\n",
        "test = pd.read_csv(os.path.join(data_path, 'test.csv'), parse_dates = ['timestamp'])\n",
        "building = pd.read_csv(os.path.join(data_path, 'building_metadata.csv'))\n",
        "weather_test = pd.read_csv(os.path.join(data_path, 'weather_test.csv'), parse_dates = ['timestamp'])\n",
        "#submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\n",
        "weather_train = pd.read_csv(os.path.join(data_path, 'weather_train.csv'), parse_dates = ['timestamp'])\n",
        "\n",
        "train = reduce_mem_usage(train, use_float16 = True)\n",
        "building = reduce_mem_usage(building, use_float16 = True)\n",
        "weather_train = reduce_mem_usage(weather_train, use_float16 = True)\n",
        "test = reduce_mem_usage(test)\n",
        "weather_test = reduce_mem_usage(weather_test)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['README.md', 'anscombe.json', 'weather_train.csv', 'weather_test.csv', 'building_metadata.csv', 'sample_submission.csv', 'train.csv', 'test.csv', 'california_housing_test.csv', 'mnist_test.csv', 'mnist_train_small.csv', 'california_housing_train.csv']\n",
            "Memory usage of dataframe is 616.95 MB\n",
            "Memory usage after optimization is: 289.19 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 0.07 MB\n",
            "Memory usage after optimization is: 0.02 MB\n",
            "Decreased by 73.8%\n",
            "Memory usage of dataframe is 9.60 MB\n",
            "Memory usage after optimization is: 3.07 MB\n",
            "Decreased by 68.1%\n",
            "Memory usage of dataframe is 1272.51 MB\n",
            "Memory usage after optimization is: 596.49 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 19.04 MB\n",
            "Memory usage after optimization is: 9.78 MB\n",
            "Decreased by 48.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vByNpOtOHUL0",
        "colab_type": "text"
      },
      "source": [
        "1. Load dataset \n",
        "2. Do EDA to analyze data structure \n",
        "3. Do feature engineering \n",
        "4. Apply model training \n",
        "5. Make test data prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXBxv4GcLzxQ",
        "colab_type": "code",
        "outputId": "df4df7e1-b52c-41cc-a94c-872994c819a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        }
      },
      "source": [
        "# Time alignment\n",
        "\n",
        "temp_skeleton = pd.concat([weather_train, weather_test], ignore_index = True)\n",
        "weather_key = ['site_id', 'timestamp']\n",
        "# Drop samples with same site and same timestamp\n",
        "temp_skeleton = temp_skeleton[weather_key + ['air_temperature']].drop_duplicates( \\\n",
        "                              subset = weather_key).sort_values(by = weather_key)\n",
        "\n",
        "# Ranking of temperature in each date, at each site\n",
        "temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton['timestamp'].dt.date]) \\\n",
        "                        ['air_temperature'].rank(method = 'average')\n",
        "\n",
        "# Create a dataframe that consisted of: site_id * hourly mean rank of temperature for 24 hours\n",
        "df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level = 1)\n",
        "\n",
        "# Subtract max temperature hourly rank by 14, get time alignment gap: time gap btw 14:00 and peak temp. timing\n",
        "site_offset = pd.Series(df_2d.values.argmax(axis = 1) - 14)\n",
        "site_offset.index.name = 'site_id'\n",
        "\n",
        "def time_align(df):\n",
        "  # create time offset column\n",
        "  df['offset'] = df.site_id.map(site_offset)\n",
        "  df['timestamp_aligned'] = df.timestamp - pd.to_timedelta(df.offset, unit = 'hour')\n",
        "  df['timestamp'] = df['timestamp_aligned']\n",
        "  del df['timestamp_aligned']\n",
        "  return df\n",
        "\n",
        "# Now, we can align weather_train, weather_test data\n",
        "weather_train = time_align(weather_train)\n",
        "weather_test = time_align(weather_test)\n",
        "\n",
        "del df_2d, temp_skeleton, site_offset\n",
        "\n",
        "# Do interpolation for weather data first, for too much missing values. There may still be some missing values after this.\n",
        "# Interpolate by each site across the timestamp\n",
        "weather_train = weather_train.groupby('site_id').apply(lambda x_site: x_site.interpolate(limit_direction = 'both'))\n",
        "weather_test = weather_test.groupby('site_id').apply(lambda x_site: x_site.interpolate(limit_direction = 'both'))\n",
        "\n",
        "print('Missing values in weather train data after interpolation: \\n')\n",
        "print(weather_train.isnull().sum().sort_values(ascending = False))\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing values in weather train data after interpolation: \n",
            "\n",
            "precip_depth_1_hr     26273\n",
            "cloud_coverage        17228\n",
            "sea_level_pressure     8755\n",
            "offset                    0\n",
            "wind_speed                0\n",
            "wind_direction            0\n",
            "dew_temperature           0\n",
            "air_temperature           0\n",
            "timestamp                 0\n",
            "site_id                   0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51enM79EN3Wv",
        "colab_type": "text"
      },
      "source": [
        "I can see that although some NaN values is filled in weather_data, interpolation couldn't fill all of the missing values.  \n",
        "We will continue this in FE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxRKgp-QaW1a",
        "colab_type": "code",
        "outputId": "53dbe262-2bde-475f-99be-b8b35601985b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Functions for several preprocessing and feature engineering\n",
        "\n",
        "# Encode cyclic features\n",
        "def encode_cyclic_feature(df, col, max_val):\n",
        "  '''\n",
        "  Encode cyclic feature with sin cosine transform\n",
        "  df: dataframe contains cyclic feature\n",
        "  col: cylcic features to transform\n",
        "  max_val: max value for that cyclic column\n",
        "  '''\n",
        "  df[col + '_sin'] = np.sin(2*np.pi*(df[col]/max_val))\n",
        "  del df[col]\n",
        "  return df\n",
        "\n",
        "# Fill NaNs\n",
        "def mean_without_overflow_fast(col):\n",
        "    # Compute mean value of each column which contains missing value\n",
        "    col /= len(col)\n",
        "    return col.mean() * len(col)\n",
        "\n",
        "def fillna(df):\n",
        "  '''\n",
        "  Fill NaN for dataframe \n",
        "  output: dataframe without missing values\n",
        "  '''\n",
        "  # pick up the columns contains null value\n",
        "  null_col = 100 - df.count()/len(df)*100\n",
        "  null_col = df.loc[:, null_col > 0] # dataframe from train that contain null columns\n",
        "  null_col_mean = null_col.apply(mean_without_overflow_fast) # mean value to fill in\n",
        "\n",
        "  for col in null_col.keys():\n",
        "    if col == 'year_built' or col == 'floor_count':\n",
        "      df[col].fillna(math.floor(null_col_mean[col]), inplace = True)\n",
        "    else:\n",
        "      df[col].fillna(null_col_mean[col], inplace = True)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Create new feature\n",
        "# Time stamp feature\n",
        "def time_transform(df):\n",
        "  df['hour'] = df['timestamp'].dt.hour\n",
        "  df['year'] = df['timestamp'].dt.year\n",
        "  df['month'] = df.timestamp.dt.month\n",
        "  df['day'] = df.timestamp.dt.day\n",
        "  df['dayofweek'] = df.timestamp.dt.dayofweek\n",
        "  \n",
        "  return df\n",
        "\n",
        "# Create is_holiday feature by US_Holiday calendar\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
        "\n",
        "# Add is_holiday = 1: The date that within USA_holiday, and the dates that is weekend\n",
        "\n",
        "date_range = pd.date_range(start = train['timestamp'].min(), end = test['timestamp'].max())\n",
        "us_holidays = calendar().holidays(start = date_range.min(), end = date_range.max()) # USA holidays within data date_range\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMs63obQid7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature engineering\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "def preprocessing(df, building, weather, is_train = False):\n",
        "  '''\n",
        "  df: train data or test dataframe\n",
        "  '''\n",
        "\n",
        "  # 0. Combine the dataset into one\n",
        "  df = df.merge(building, on = 'building_id', how = 'left')\n",
        "  df = df.merge(weather, on = ['site_id', 'timestamp'], how = 'left')\n",
        "\n",
        "  weather_col = ['air_temperature', 'dew_temperature', 'wind_speed', \\\n",
        "               'wind_direction', 'sea_level_pressure',\n",
        "               'precip_depth_1_hr', 'cloud_coverage']\n",
        "  df = df.dropna(subset = weather_col, how = 'all')\n",
        "\n",
        "  # 1. Create new features\n",
        "  df = time_transform(df)\n",
        "\n",
        "  # Only datetime64 could apply isin() function, which is convenient\n",
        "  df['is_holiday'] = (df['timestamp'].dt.date).astype('datetime64').isin(us_holidays).astype(np.int8)\n",
        "  df.loc[(df['timestamp'].dt.dayofweek == 5) | (df['timestamp'].dt.dayofweek == 6), 'is_holiday'] = 1\n",
        "\n",
        "  # 2. Data transformation to make data better for prediction\n",
        "  # Log transformation for numerical data\n",
        "  if is_train:\n",
        "    df['meter_reading'] = np.log1p(df['meter_reading'])\n",
        "  \n",
        "  df['square_feet'] = np.log1p(df['square_feet'])\n",
        "\n",
        "  # Encode cyclic features\n",
        "  df = encode_cyclic_feature(df, 'dayofweek', 7)\n",
        "  df = encode_cyclic_feature(df, 'hour', 24)\n",
        "  df = encode_cyclic_feature(df, 'day', 31)\n",
        "  df = encode_cyclic_feature(df, 'month', 31)\n",
        "  \n",
        "  # 3. Fill NaNs\n",
        "  df = fillna(df)\n",
        "\n",
        "  # 4. Categorical encoding\n",
        "  df['primary_use'] = le.fit_transform(df['primary_use'])\n",
        "\n",
        "  # 5. Data cleaning: NaN rows, Outliers, ...etc\n",
        "  drop_features = ['wind_speed', 'sea_level_pressure', 'wind_direction', 'timestamp']\n",
        "  df.drop(drop_features, axis = 1, inplace = True)\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnMVmJvdFQ56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = preprocessing(train, building, weather_train, is_train = True)\n",
        "test = preprocessing(test, building, weather_test, is_train = False)\n",
        "\n",
        "del building, weather_train, weather_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny7p-fLAypJV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2077c0f-81c6-436c-a79e-fa21d29cd063"
      },
      "source": [
        "y_train = train['meter_reading']\n",
        "x_train = train.drop('meter_reading', axis = 1, inplace = False)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccRe5dT_Pl-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Boosting model\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size = 0.2, \n",
        "                                            random_state = 9)\n",
        "\n",
        "params = {\n",
        "    'n_estimators':400,\n",
        "    'max_depth':30,\n",
        "    'num_leaves':40\n",
        "}\n",
        "\n",
        "lgb0 = lgb.LGBMRegressor(params\n",
        "                         )\n",
        "\n",
        "lgb0.fit(x_tr, y_tr,\n",
        "         eval_metric = 'mae',\n",
        "         eval_set = (x_val, y_val),\n",
        "         early_stopping_rounds = 100,\n",
        "         verbose = 50)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhxsLLXJQNum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ba063e74-265a-4461-edad-1f4b80958d74"
      },
      "source": [
        "lgb0 = lgb.LGBMRegressor(**params\n",
        "                         )\n",
        "\n",
        "lgb0.fit(x_tr, y_tr,\n",
        "         eval_metric = 'mae',\n",
        "         eval_set = (x_val, y_val),\n",
        "         early_stopping_rounds = 100,\n",
        "         verbose = 50)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[50]\tvalid_0's l1: 0.961707\tvalid_0's l2: 1.89192\n",
            "[100]\tvalid_0's l1: 0.857458\tvalid_0's l2: 1.57749\n",
            "[150]\tvalid_0's l1: 0.795146\tvalid_0's l2: 1.40189\n",
            "[200]\tvalid_0's l1: 0.747202\tvalid_0's l2: 1.27137\n",
            "[250]\tvalid_0's l1: 0.710961\tvalid_0's l2: 1.1792\n",
            "[300]\tvalid_0's l1: 0.681345\tvalid_0's l2: 1.10051\n",
            "[350]\tvalid_0's l1: 0.655748\tvalid_0's l2: 1.02977\n",
            "[400]\tvalid_0's l1: 0.635627\tvalid_0's l2: 0.984963\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[400]\tvalid_0's l1: 0.635627\tvalid_0's l2: 0.984963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "620"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt0dKJxsxhym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "c1537336-3ba1-418d-84cf-45fa74d5bdcc"
      },
      "source": [
        "# Stacking ensemble implemented by: https://www.kaggle.com/mubashir44/simple-ensemble-model-stacking\n",
        "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size = 0.2, \n",
        "                                            random_state = 10)\n",
        "\n",
        "\n",
        "class sklearnhelper(object):\n",
        "  '''\n",
        "  Helper that defined sklearn model in a general way, which we can stack them together for ensemble later\n",
        "  '''\n",
        "  def __init__(self, estimator, params = None, seed = 0):\n",
        "    params['random_state'] = seed\n",
        "    self.estimator = estimator(**params)\n",
        "\n",
        "  def train(self, x_train, y_train):\n",
        "    self.estimator.fit(x_train, y_train)\n",
        "\n",
        "  def predict(self, x):\n",
        "    return self.estimator.predict(x)\n",
        "\n",
        "  def feature_importance(self, x, y):\n",
        "    # print feature importance for model fit with x & y\n",
        "    print(self.estimator.fit(x, y).feature_importances_)\n",
        "\n",
        "n_folds = 5\n",
        "kfold = sklearn.model_selection.KFold(n_splits = n_folds, random_state = 9)\n",
        "\n",
        "def get_oof(estimator, x_train, y_train, x_test):\n",
        "  '''\n",
        "  Train data on level 1 baseline model, and return hold-out set prediction for level 2 meta-model\n",
        "  I don't know why predict x_test in this step\n",
        "  x_train: train data\n",
        "  x_test: test data\n",
        "  '''\n",
        "  oof_train = np.zeros((len(x_train),)) # store predictions by base_model for train data\n",
        "  oof_test = np.zeros((len(x_test),))  # mean test data prediction by single base model\n",
        "  oof_test_skf = np.zeros((n_folds, len(x_test))) # test data prediction made by each i-th fold trained base model\n",
        "\n",
        "  i = 0\n",
        "  for train_id, valid_id in kfold.split(x_train):\n",
        "    x_tr = x_train[train_id]\n",
        "    y_tr = y_train[train_id]\n",
        "    x_oof = x_train[valid_id] # hold out data\n",
        "\n",
        "    estimator.fit(x_tr, y_tr) # train on k-1 fold data\n",
        "    oof_train[valid_id] = estimator.predict(x_oof) # combine each 1 fold hold-out data into full prediction of train data\n",
        "    oof_test_skf[i, :] = estimator.predict(x_test)\n",
        "    i += 1\n",
        "\n",
        "  oof_test = oof_test_skf.mean(axis = 0) # mean prediction for each fold trained model\n",
        "  return oof_train.reshape((-1, 1)), oof_test.reshape((-1, 1))\n",
        "\n",
        "\n",
        "# Define base model: random forest, extratree regressor, lightgbm regressor\n",
        "\n",
        "rf_param = {\n",
        "    'n_estimators':100,\n",
        "    'max_depth':10,\n",
        "    'verbose':0,\n",
        "    'min_samples_leaf':2\n",
        "}\n",
        "\n",
        "lgb_param = {\n",
        "    'n_estimators':400,\n",
        "    'num_leaves':40,\n",
        "    'max_depth':7,\n",
        "    'sub_sample':0.7\n",
        "}\n",
        "\n",
        "et_param = {\n",
        "    'n_jobs':-1,\n",
        "    'n_estimators':100,\n",
        "    'max_features':0.5,\n",
        "    'max_depth':12,\n",
        "    'min_samples_leaf':2\n",
        "}\n",
        "\n",
        "# Define level 0 base model\n",
        "seed = 9\n",
        "rf = sklearnhelper(RandomForestRegressor(), rf_param, seed)\n",
        "lgbm = lgb.LGBMRegressor(**lgb_param, random_state = seed)\n",
        "et = sklearnhelper(ExtraTreeRegressor(), et_param, seed)\n",
        "\n",
        "# Stacking: level 1 base line model training\n",
        "rf_oof_train, rf_oof_test = get_oof(rf, x_train, y_train, x_test)\n",
        "lgb_oof_train, lgb_oof_test = get_oof(lgbm, x_train, y_train, x_test)\n",
        "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test)\n",
        "\n",
        "# metao model input: concatenate the predictions from baseline model\n",
        "x_meta = np.concatenate((rf_oof_train, lgb_oof_train, et_oof_train), axis = 1) \n",
        "y_meta = y_train\n",
        "test_meta = np.concatenate((rf_oof_test, lgb_oof_test, et_oof_test), axis = 1)\n",
        "\n",
        "lgb_param = {\n",
        "    'n_estimators':400,\n",
        "    'num_leaves':40,\n",
        "    'max_depth':7,\n",
        "    'sub_sample':0.7\n",
        "}\n",
        "\n",
        "meta_model = lgb.LGBMRegressor(**lgb_param)\n",
        "\n",
        "# Train meta model\n",
        "meta_model.fit(x_meta, y_meta,\n",
        "               eval_metric = 'mae',\n",
        "               eval_set = (x_val, y_val),\n",
        "               early_stopping_rounds = 100,\n",
        "               verbose = 50)\n",
        "\n",
        "prediction = meta_model.predict(test_meta)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e65c04cab120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Define level 0 base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearnhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mlgbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlgb_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0met\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearnhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtraTreeRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-e65c04cab120>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, params, seed)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'random_state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'RandomForestRegressor' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTlqmj-eUXMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestRegressor(**rf_param)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}