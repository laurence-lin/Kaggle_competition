{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ashrae_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurence-lin/Kaggle_competition/blob/master/Ashrae_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Vn9mfylThK",
        "colab_type": "code",
        "outputId": "b03a8305-88a0-454c-bab8-2cb08fbf7e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import sklearn\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "import gc\n",
        "from google.colab import files\n",
        "# load data from Cloud Storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Configure GCP project and use gsutil to copy the file from storage\n",
        "\n",
        "!gcloud config set project 'blind-detection'\n",
        "!gsutil -m cp -r gs://ashare_dataset/*.csv  sample_data/\n",
        "\n",
        "\n",
        "\n",
        "# Reduce memory function\n",
        "\n",
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16\n",
        "\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# Load all datasets and reduce memory\n",
        "print(os.listdir('sample_data/'))\n",
        "data_path = 'sample_data/'\n",
        "train = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates = ['timestamp'])\n",
        "test = pd.read_csv(os.path.join(data_path, 'test.csv'), parse_dates = ['timestamp'])\n",
        "building = pd.read_csv(os.path.join(data_path, 'building_metadata.csv'))\n",
        "weather_test = pd.read_csv(os.path.join(data_path, 'weather_test.csv'), parse_dates = ['timestamp'])\n",
        "#submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\n",
        "weather_train = pd.read_csv(os.path.join(data_path, 'weather_train.csv'), parse_dates = ['timestamp'])\n",
        "\n",
        "train = reduce_mem_usage(train, use_float16 = True)\n",
        "building = reduce_mem_usage(building, use_float16 = True)\n",
        "weather_train = reduce_mem_usage(weather_train, use_float16 = True)\n",
        "test = reduce_mem_usage(test)\n",
        "weather_test = reduce_mem_usage(weather_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://ashare_dataset/sample_submission.csv...\n",
            "Copying gs://ashare_dataset/building_metadata.csv...\n",
            "Copying gs://ashare_dataset/test.csv...\n",
            "Copying gs://ashare_dataset/train.csv...\n",
            "Copying gs://ashare_dataset/weather_test.csv...\n",
            "Copying gs://ashare_dataset/weather_train.csv...\n",
            "\\ [6/6 files][  2.4 GiB/  2.4 GiB] 100% Done  17.4 MiB/s ETA 00:00:00           \n",
            "Operation completed over 6 objects/2.4 GiB.                                      \n",
            "['README.md', 'anscombe.json', 'train.csv', 'weather_train.csv', 'building_metadata.csv', 'weather_test.csv', 'test.csv', 'sample_submission.csv', 'mnist_train_small.csv', 'mnist_test.csv', 'california_housing_train.csv', 'california_housing_test.csv']\n",
            "Memory usage of dataframe is 616.95 MB\n",
            "Memory usage after optimization is: 289.19 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 0.07 MB\n",
            "Memory usage after optimization is: 0.02 MB\n",
            "Decreased by 73.8%\n",
            "Memory usage of dataframe is 9.60 MB\n",
            "Memory usage after optimization is: 3.07 MB\n",
            "Decreased by 68.1%\n",
            "Memory usage of dataframe is 1272.51 MB\n",
            "Memory usage after optimization is: 596.49 MB\n",
            "Decreased by 53.1%\n",
            "Memory usage of dataframe is 19.04 MB\n",
            "Memory usage after optimization is: 9.78 MB\n",
            "Decreased by 48.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YQnJQf3jMYz",
        "colab_type": "code",
        "outputId": "99e2286a-c4b7-464d-8e72-75c071dedbfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "# EDA: Data analysis\n",
        "\n",
        "\n",
        "# Observe target distribution\n",
        "'''\n",
        "plt.scatter(range(train.shape[0]), np.sort(train['meter_reading'].values))\n",
        "plt.xlabel('Index of all measurements')\n",
        "plt.ylabel('Meter Reading Values')\n",
        "plt.title('Target Distributions: Increasing')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#plt.plot(train['meter_reading'])\n",
        "plt.hist(train['meter_reading'], \n",
        "         bins = 25, #number of histograms to show up\n",
        "         )\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Observe missing values or zeros\n",
        "# train data\n",
        "# Should deal with missing values here\n",
        "def check_missing(df):\n",
        "  total = df.isnull().sum().sort_values(ascending = False) # missing values in each feature\n",
        "  percent = total / df.shape[0] * 100 # percentage of missing values in each feature\n",
        "  missing_in_data = pd.concat([total, percent], axis = 1, keys = ['Total', 'Percentage'])\n",
        "  print(missing_in_data)\n",
        "\n",
        "#print('Training missing: \\n')\n",
        "#check_missing(train)\n",
        "\n",
        "print('Weather train missing: \\n')\n",
        "check_missing(weather_train)\n",
        "\n",
        "print('Weather test missing: \\n')\n",
        "check_missing(weather_test)\n",
        "\n",
        "print('Building meta missing: \\n')\n",
        "check_missing(building)\n",
        "\n",
        "# Check feature correlation with target\n",
        "# Should do feature selection here\n",
        "corr_ = train.corr(method = 'pearson')\n",
        "print('Most positive correlations:\\n', corr_['meter_reading'].sort_values(ascending = False))\n",
        "import seaborn as sns\n",
        "sns.heatmap(corr_,\n",
        "            annot = True, # write data value in each cell \n",
        "            cmap = plt.cm.RdYlBu_r, # set the color map \n",
        "            vmin = -0.25)\n",
        "plt.title('Correlation heatmap')\n",
        "\n",
        "\n",
        "\n",
        "# Show numerical feature distribution\n",
        "# Should do feature transformation or processing\n",
        "\n",
        "def plot_dist_curve(column):\n",
        "  \n",
        "  #Plot dist curve for weather_train & test data for given column name\n",
        "  \n",
        "  # I don't see the advantage dist better than hist\n",
        "  fig, ax = plt.subplots(figsize = [10, 10]) # ax contains the figure elements\n",
        "  sns.distplot(weather_train[column].dropna(), ax = ax).set_title(column)\n",
        "  sns.distplot(weather_test[column].dropna(), ax = ax)\n",
        "  plt.xlabel(column, fontsize = 15)\n",
        "  plt.legend(['train', 'test']) # legend show by order of plot\n",
        "  plt.show()\n",
        "\n",
        "#plot_dist_curve('air_temperature')\n",
        "\n",
        "# Outlier distribution\n",
        "# Should detect outlier and abandon the outliers from dataset\n",
        "# Outlier: the outlier of (mean value of all meter readings along the timestamp)\n",
        "fig1, ax1 = plt.subplots()\n",
        "y_mean_eachtime = train.groupby('timestamp').meter_reading.mean() # plot the mean value of all meter sensors in each time date\n",
        "print(type(y_mean_eachtime))\n",
        "#plt.plot(y_mean_eachtime.values)\n",
        "y_mean_eachtime.plot(figsize = (20, 8), ax = ax1)\n",
        "\n",
        "y_mean_eachtime.rolling(window = 10).std().plot(figsize = (20, 8))\n",
        "ax1.axhline(y = .009, color = 'red')\n",
        "ax1.legend(['Mean value of meter_reading each date', 'std of 10 date', 'horizontal'])\n",
        "'''\n",
        "\n",
        "\n",
        "#print(weather_train.isnull().sum())\n",
        "#print(building.isnull().sum())\n",
        "temp = train.merge(building, on = 'building_id', how = 'left')\n",
        "show = temp.groupby('primary_use').size().reset_index(name = 'count')\n",
        "plt.figure()\n",
        "ax = sns.barplot(show['primary_use'], show['count'])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 50, ha = 'right')\n",
        "del temp\n",
        "\n",
        "print('Meter type distribution: ')\n",
        "show_ = train.groupby('meter').size().reset_index(name = 'count')\n",
        "plt.figure(2)\n",
        "ax2 = sns.barplot(show_['meter'], show_['count'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Meter type distribution: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAETCAYAAAA7wAFvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqXEGgSQ1ziCQpMYZBJLUuP8Hk0qJXSOa2AsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNFYTFM5cim9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = train.merge(building, on = 'building_id', how = 'left')\n",
        "temp = temp.merge(weather_train, on = ['site_id', 'timestamp'], how = 'left')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s6nPcGCanD",
        "colab_type": "code",
        "outputId": "add2ed46-1a70-40de-fbca-a3e96707adf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "source": [
        "# Fill missing values with mean() of each feature\n",
        "\n",
        "def fill_missing(df):\n",
        "  '''\n",
        "  Fill missing values for numerical features.\n",
        "  Fill in with mean() of each column\n",
        "  '''\n",
        "\n",
        "  def mean_without_nan(col):\n",
        "    # ignore NaN when computing mean in column\n",
        "    col /= len(col)\n",
        "    return col.mean() * len(col)\n",
        "  \n",
        "  missing_feature = df.isnull().sum()[df.isnull().sum() > 0]\n",
        "  missing_feature = list(missing_feature.index)\n",
        "  feature_mean = df.loc[:, missing_feature].apply(mean_without_nan)\n",
        "\n",
        "  for feature in missing_feature:\n",
        "    if feature == 'year_built' or feature == 'floor_count':\n",
        "      df[feature].fillna(math.floor(feature_mean[feature]), inplace = True)\n",
        "    else:\n",
        "      df[feature].fillna(feature_mean[feature], inplace = True)\n",
        "\n",
        "  return df\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing feature count: \n",
            "\n",
            "year_built            12127645\n",
            "floor_count           16709167\n",
            "air_temperature          96658\n",
            "cloud_coverage         8825365\n",
            "dew_temperature         100140\n",
            "precip_depth_1_hr      3749023\n",
            "sea_level_pressure     1231669\n",
            "wind_direction         1449048\n",
            "wind_speed              143676\n",
            "dtype: int64\n",
            "building_id           0\n",
            "meter                 0\n",
            "timestamp             0\n",
            "meter_reading         0\n",
            "site_id               0\n",
            "primary_use           0\n",
            "square_feet           0\n",
            "year_built            0\n",
            "floor_count           0\n",
            "air_temperature       0\n",
            "cloud_coverage        0\n",
            "dew_temperature       0\n",
            "precip_depth_1_hr     0\n",
            "sea_level_pressure    0\n",
            "wind_direction        0\n",
            "wind_speed            0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFzpoW8ebuX8",
        "colab_type": "text"
      },
      "source": [
        "As to training data, I've about 1448 buildings to predict. \n",
        "Time scan over 3 years, each day as time step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFvEyyEfbzt7",
        "colab_type": "text"
      },
      "source": [
        "Now should we rearrange the data? Each building locate in single site.\n",
        "Assumption to rearrange:\n",
        "1. Sort by building_id\n",
        "2. Sort by time_stamp\n",
        "\n",
        "Feature engineering:\n",
        "Data cleaning: delete useless column\n",
        "Data processing: standardization\n",
        "Feature engineering: create new features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvX0H7mEb0nO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After EDA, do feature engineering and data cleaning\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocessing(x, building_data, weather_data, test = False):\n",
        "    '''\n",
        "    Preprocessing for training and testing data:\n",
        "    Merge data with other two feature dataset\n",
        "    Create new useful features\n",
        "    Sort data by timestamp\n",
        "    \n",
        "    Return:\n",
        "    training: X & y\n",
        "    testing: X\n",
        "    '''\n",
        "\n",
        "    # Fill missing value\n",
        "    # 1. Fill NaN along the time series data in each site respectively: Because timestamp in weather is consecutive\n",
        "    weather_data = weather_data.groupby('timestamp').apply(lambda group: group.interpolate(limit_direction='both'))\n",
        "\n",
        "    # Merge all feature data\n",
        "    X = x.merge(building_data, on = 'building_id', how = 'left')\n",
        "    X = X.merge(weather_data, on = ['site_id', 'timestamp'], how = 'left')\n",
        "    X = fill_missing(X)\n",
        "\n",
        "    # Categorical feature transformation\n",
        "    X['primary_use'] = le.fit_transform(X['primary_use'])\n",
        "\n",
        "    # Feature generation\n",
        "    # 3 time feature: hour, weekday, in_holiday\n",
        "    # Could validate this later\n",
        "    X['timestamp'] = pd.to_datetime(X['timestamp'], format = '%Y-%m-%d %H:%M:%S')\n",
        "    X['square_feet'] = np.log1p(X['square_feet'])\n",
        "    # Sort training data by timestamp\n",
        "    if not test:\n",
        "        X.sort_values(by = 'timestamp', inplace = True) # sort train data by time\n",
        "        X.reset_index(drop = True, inplace = True)  # reset index to default that messed by sorting\n",
        "    \n",
        "    X['hour'] = X['timestamp'].dt.hour\n",
        "    X['weekday'] = X['timestamp'].dt.dayofweek\n",
        "    \n",
        "    # Data Cleaning: drop useless features\n",
        "    X.drop(['timestamp', 'sea_level_pressure', 'wind_direction', 'wind_speed'], axis = 1, inplace = True)\n",
        "    \n",
        "    # Return clean data: X & y\n",
        "    if not test:\n",
        "        y = np.log1p(X['meter_reading'])\n",
        "        X.drop('meter_reading', axis = 1, inplace = True)\n",
        "        return X, y\n",
        "    elif test == True:\n",
        "        row_id = X['row_id']\n",
        "        X.drop('row_id', axis = 1, inplace = True)\n",
        "        return X, row_id\n",
        "    \n",
        "## Q1: Should I do log transformation?\n",
        "## Q3: There are still columns contains multiple missing values\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwT3y4g5PeoM",
        "colab_type": "code",
        "outputId": "7bd35b58-55c9-4ace-8080-69f01ea8cf91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "# Stacking ensemble model\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create training data\n",
        "x_data, y_data = preprocessing(train, building, weather_train, False)\n",
        "del train, weather_train\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = train_test_split(x_data, y_data, test_size = 0.3)\n",
        "\n",
        "def stack_model():\n",
        "  \n",
        "\n",
        "\n",
        "estimators = [('lr', RidgeCV(random_state = 39)),\n",
        "             ('randomforest', RandomForestRegressor(random_state = 39)),\n",
        "             ('randomforest2', RandomForestRegressor(random_state = 42))\n",
        "             ]\n",
        "\n",
        "\n",
        "# Create preprocessing & model training pipeline\n",
        "pipe = Pipeline(['preprocessing', preprocessing(), 'estimator', stack_model()])\n",
        "#pipe.fit()\n",
        "#pipe.score()\n",
        "\n",
        "stack_model.fit(x_train, y_train,\n",
        "                eval_metric = 'rmse',\n",
        "                early_stopping_rounds = 200,\n",
        "                verbose = 200\n",
        "                )\n",
        "print('Stacking score:', stack_model.score(x_valid, y_valid))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c165adb63549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRidgeCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStackingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m estimators = [('lr', RidgeCV(random_state = 39)),\n\u001b[1;32m      5\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0;34m'randomforest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'StackingRegressor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTU5HTp_O2GL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LGBM ensemble\n",
        "# Use GridsearchCV to search best parameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from ligthgbm import LGBMRegressor\n",
        "\n",
        "# Search on subset first!?\n",
        "# param grid to search for best parameters\n",
        "param_grid = {\n",
        "    'num_leaves':[30, 45, 60],\n",
        "    'max_depth':[5, 10, 20]\n",
        "}\n",
        "\n",
        "lgb = LGBMRegressor('boosting_type':'gbdt',\n",
        "    'learning_rate':0.1,\n",
        "    'n_estimators':800,\n",
        "    'reg_lambda':2)\n",
        "\n",
        "gridsearch= GridSearchCV(lgb, \n",
        "                         param_grid, \n",
        "                         'rmse') # score to select parameters\n",
        "gridsearch.fit(x, y)\n",
        "print('Best parameters selected: \\n', gridsearch.best_params_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uveeT7k3wNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the whole train set by CV with best parameters selected\n",
        "lgb_final = gridsearch.best_estimator_\n",
        "del lgb\n",
        "lgb_final.fit(\n",
        "    x_train, y_train,\n",
        "    eval_metric = 'rmse',\n",
        "    early_stopping_rounds = 200,\n",
        "    verbose = 200\n",
        ")\n",
        "\n",
        "print('LGBM score:', lgb.score(x_valid, y_valid))\n",
        "\n",
        "\n",
        "del x_train, x_valid, y_train, y_valid, train_set, valid_set\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16tLIaNicLAM",
        "colab_type": "code",
        "outputId": "4cd3e325-79bf-4bc5-8001-08459a87ce18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Prediction on testing data\n",
        "test_data, row_ids = preprocessing(test, building, weather_test, True)\n",
        "\n",
        "del test, weather_test, building\n",
        "gc.collect()\n",
        "\n",
        "# Scoring test data\n",
        "prediction = train_booster.predict(data = test_data \n",
        "                                   #num_iteration = train_booster.best_iteration\n",
        "                                  )\n",
        "# Transform back from the log1p transformation on the y_data, because our model predict on log-transformation target\n",
        "prediction = np.expm1(prediction)\n",
        "# limit the value minimum to zero, no negative\n",
        "prediction = np.clip(prediction, a_min = 0, a_max = None)\n",
        "\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    0\n",
            "1    1\n",
            "2    2\n",
            "3    3\n",
            "4    4\n",
            "Name: row_id, dtype: int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21NQKgSWSeAU",
        "colab_type": "code",
        "outputId": "cee37779-0f7b-4de0-a5f4-783270a63240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightgbm.basic.Booster at 0x7f1eabcbb320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HCWLs4yTyxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cross_validation import KFold\n",
        "import numpy as np\n",
        "\n",
        "# ensemble: stacking\n",
        "class Ensemble(object):\n",
        "  def __init__(self, n_fold, stacker, base_model):\n",
        "      '''\n",
        "      n_fold: N-fold cross stacking, n-1 for training, 1 fold for predicting\n",
        "      stacker: meta model for final output\n",
        "      base_model: base model as weak learner to train on n-1 fold data\n",
        "      '''\n",
        "      self.n_fold = n_fold\n",
        "      self.stacker = stacker\n",
        "      self.base_model = base_model # list of base models, could be homogeneous\n",
        "  \n",
        "  def fit_predict(self, X, y, T):\n",
        "    '''\n",
        "    fit all training data with label y, and make predictions on testing data T\n",
        "    Input: training data X, and labels y\n",
        "    output: prediction on testing data T, without labels\n",
        "    '''\n",
        "    # transform X & Y & T to array structure\n",
        "    X = np.array(X) # train data\n",
        "    y = np.array(y) # train labels\n",
        "    T = np.array(T) # test data\n",
        "\n",
        "    folds = list(KFold(\n",
        "        len(y), # number of training samples\n",
        "        n_fold = self.n_fold, \n",
        "        shuffle = True, # shuffle data before split to batches\n",
        "        random_state = 9 # we could get same random sample by same training data later\n",
        "    ))\n",
        "\n",
        "    s_train = np.zeros(X.shape[0], len(self.base_model)) # input for meta model\n",
        "    s_test = np.zeros(X.shape[0], len(self.base_model))  # test data prediction by each base model\n",
        "\n",
        "    for i, clf in enumerate(self.base_model):\n",
        "      s_test_i = np.zeros(T.shape[0], len(self.n_fold)) # to store predictions on testing data in each fold\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}